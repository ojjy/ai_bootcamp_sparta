{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbgz49PvHhLt"
   },
   "source": [
    "# DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
    "\n",
    "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LqgujQUbv6X",
    "outputId": "f8faaad2-ac05-401b-c8f1-45ec07cbb71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.11/site-packages (1.28.64)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.11/site-packages (2023.10.3)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.64 in /opt/anaconda3/lib/python3.11/site-packages (from boto3) (1.31.64)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from boto3) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.11/site-packages (from botocore<1.32.0,>=1.31.64->boto3) (2.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.64->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YP3FxG9IF7O"
   },
   "source": [
    "### [MY CODE] 데이터셋 로드 및 전처리\n",
    "- AG_News 데이터셋 로드 2) 학습에 사용하기 위해 텍스트와 레이블 데이터를 전처리\n",
    "- collate_fn 함수: 배치 데이터의 텍스트를 패딩 처리하여 고정된 크기의 입력으로 변환합니다\n",
    "- DataLoader: 데이터셋을 배치 크기 단위로 나눠 학습과 테스트에서 사용할 수 있도록 준비합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lGiZUoPby6e",
    "outputId": "592b19ee-1892-4821-fcef-24fe40cc185f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd40808b8fdf4cd5905a3215987d5b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9dbec5f5154db8a4a4a6d392c24e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfab4baa6535450abca980d7d0d0c6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1618155c774afa9e0d1e53bbb5af51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58e25ba920147fd95c9478f7b6c3ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kelly/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# AG_News 데이터셋 로드\n",
    "# - Huggingface의 fancyzhx/ag_news 데이터셋 로드(뉴스 기사를 4개의 클래스로 분류하는 데 사용)\n",
    "ds = load_dataset('fancyzhx/ag_news')\n",
    "\n",
    "# DistilBERT 토크나이저 초기화\n",
    "# - Huggingface에서 제공하는 DistilBERT 토크나이저를 로드\n",
    "# - 이 토크나이저는 DistilBERT 모델과 동일한 토큰화를 수행하며, 소문자 변환 및 WordPiece 방식을 사용\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')\n",
    "\n",
    "# Collate 함수 수정 (truncation 제거)\n",
    "# - truncation 옵션을 제거하고, 입력 텍스트의 길이에 제한을 두지 않고 패딩 처리만 수행\n",
    "def collate_fn(batch):\n",
    "    texts, labels = [], []  # 텍스트와 레이블을 저장할 리스트 초기화\n",
    "    for row in batch:\n",
    "        labels.append(row['label']) # 데이터셋에서 각 행(row)의 'label' 값을 레이블 리스트에 추가\n",
    "        texts.append(row['text']) # 데이터셋에서 각 행(row)의 'text' 값을 텍스트 리스트에 추가\n",
    "\n",
    "    # 텍스트 데이터를 DistilBERT 토크나이저를 사용해 토큰화하고, 패딩 처리된 input_ids를 PyTorch LongTensor로 변환\n",
    "    texts = torch.LongTensor(tokenizer(texts, padding=True).input_ids)\n",
    "    # 레이블 리스트를 PyTorch LongTensor로 변환\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    # 텍스트와 레이블 텐서 반환\n",
    "    return texts, labels\n",
    "\n",
    "# DataLoader 설정\n",
    "# 학습 데이터셋을 DataLoader로 변환\n",
    "# - 배치 크기(batch_size): 64\n",
    "# - 데이터 셔플(shuffle): True (데이터 순서를 랜덤화)\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 테스트 데이터셋을 DataLoader로 변환\n",
    "# - 배치 크기(batch_size): 64\n",
    "# - 데이터 셔플(shuffle): False (테스트 데이터는 순서 유지)\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 이후 TextClassifier를 정의하고 model을 다시 초기화하므로, 초기 DistilBERT 모델(model)은 실제로 학습이나 평가에 사용되지 않음\n",
    "# model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvfl_uFLIMWO"
   },
   "source": [
    "### [MY CODE] DistilBERT 모델 설정\n",
    "- DistilBERT를 기반으로 한 뉴스 기사 분류 모델을 정의합니다.\n",
    "- DistilBERT는 텍스트 임베딩을 생성하며, 마지막 [CLS] 토큰의 출력을 사용하여 분류 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rE-y8sY9HuwP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kelly/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TextClassifier 정의\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # DistilBERT 모델 로드\n",
    "        # huggingface/pytorch-transformers에서 DistilBERT 사전 학습 모델을 로드합니다.\n",
    "        self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "        # 출력 레이어 정의\n",
    "        # DistilBERT의 마지막 출력 크기(768)를 4개의 클래스에 매핑하는 선형 레이어를 정의합니다.\n",
    "        self.classifier = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DistilBERT 모델을 통해 입력 텍스트의 임베딩을 생성\n",
    "        x = self.encoder(x)['last_hidden_state']\n",
    "        # [CLS] 토큰의 출력(x[:, 0])을 사용하여 최종 분류 결과 생성\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x\n",
    "\n",
    "# TextClassifier 인스턴스 생성\n",
    "model = TextClassifier()\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF34XkoYIeEm"
   },
   "source": [
    "### [MY CODE] DistilBERT 모델 파라미터 고정\n",
    "- 사전 학습된 DistilBERT의 가중치를 고정하여 학습 중 업데이트되지 않도록 설정합니다.\n",
    "- 이는 파라미터 고정을 통해 분류 레이어만 학습되도록 하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJaUp2Vob0U-",
    "outputId": "4cabca2b-06ce-480c-d52a-1381a955464b"
   },
   "outputs": [],
   "source": [
    "# DistilBERT의 파라미터 고정\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh-tqY8WInQt"
   },
   "source": [
    "### [MY CODE] 손실함수 및 Acccuracy 함수 정의\n",
    "- 뉴스 기사 분류 문제는 binary classification이 아닌 일반적인 classification 문제이므로 CrossEntropyLoss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xW7ETZQzzNp2",
    "outputId": "acae0d36-0b4a-4c7c-a0cd-5171e7158cf2"
   },
   "outputs": [],
   "source": [
    "# 손실 함수 및 옵티마이저 설정\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 정확도 함수 정의\n",
    "def accuracy(model, dataloader):\n",
    "    cnt = 0  # 전체 데이터 샘플 수를 저장할 변수\n",
    "    acc = 0  # 정확히 예측된 샘플 수를 저장할 변수\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data # DataLoader에서 배치 가져오기\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda' ) # 입력 데이터, 레이블을 GPU로 이동\n",
    "\n",
    "        preds = model(inputs) # 모델을 사용하여 예측값 계산\n",
    "        preds = torch.argmax(preds, dim=-1) # 다중 클래스 분류에서는 가장 높은 확률의 클래스 선택 (argmax)\n",
    "\n",
    "        # 정확히 예측된 샘플 수를 누적\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    # 전체 데이터 중 정확히 예측된 비율 반환\n",
    "    return acc / cnt\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hFvSis0JLju"
   },
   "source": [
    "### [MY CODE] 학습 및 테스트 루프\n",
    "1. 학습 루프:\n",
    "- 모델을 train 모드로 설정하고, 데이터 배치를 반복하면서 손실을 계산합니다.\n",
    "- 역전파(loss.backward())를 통해 그래디언트를 계산하고, 옵티마이저를 사용해 가중치를 업데이트합니다.\n",
    "- 매 epoch마다 평균 손실 값을 출력합니다.\n",
    "\n",
    "2. 테스트 루프:\n",
    "- 모델을 eval 모드로 설정하여 평가를 수행합니다.\n",
    "- 배치 단위로 테스트 데이터를 처리하며, 예측값과 실제값을 비교해 정확도를 계산합니다.\n",
    "- 테스트 데이터에 대한 정확도를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyTciaPZ0KYo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10  # 학습 epoch 수\n",
    "train_losses = []  # 각 epoch의 학습 손실을 저장하는 리스트\n",
    "train_accuracies = []  # 각 epoch의 학습 정확도를 저장하는 리스트\n",
    "test_accuracies = []  # 각 epoch의 테스트 정확도를 저장하는 리스트\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    total_loss = 0  # 한 epoch 동안의 총 손실\n",
    "\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()  # 모델의 기울기 초기화\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to('cpu'), labels.to('cpu')  # 데이터와 레이블을 GPU로 이동\n",
    "\n",
    "        preds = model(inputs) # 모델을 통해 예측값 계산\n",
    "        loss = loss_fn(preds, labels) # 손실 함수 계산 (예측값과 실제 레이블 비교)\n",
    "        loss.backward() # 역전파를 통해 기울기 계산\n",
    "        optimizer.step() # 옵티마이저를 통해 파라미터 업데이트\n",
    "\n",
    "        # 현재 배치의 손실을 총 손실에 누적\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 학습 정확도 저장\n",
    "    train_accuracy = accuracy(model, train_loader)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    # train_losses.append(total_loss)\n",
    "\n",
    "    # 테스트 데이터의 정확도 계산\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # 모델을 평가 모드로 전환\n",
    "        test_accuracy = accuracy(model, test_loader)  # 테스트 데이터 정확도 계산\n",
    "        test_accuracies.append(test_accuracy)  # 정확도 저장\n",
    "\n",
    "    # 한 epoch의 결과 출력\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} | Train Loss: {total_loss:.4f} | \"\n",
    "          f\"Train Accuracy: {train_accuracy:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # 학습 손실 저장\n",
    "    # 데이터셋의 크기와 배치 크기(batch size)에 따라 한 epoch에서의 배치 개수(len(train_loader))가 달라질 수 있으므로 평균으로 계산\n",
    "    train_losses.append(total_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU7BWEbgJeKm"
   },
   "source": [
    "### [MY CODE] 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvvaAEwCznt-",
    "outputId": "3363b8ca-7695-493f-96a0-5aa6b52d1d60"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to('cpu')\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cpu'), labels.to('cpu').float()\n",
    "\n",
    "    preds = model(inputs)[..., 0]\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjphVwXL00E2",
    "outputId": "7526ec71-f015-4f26-8035-3091ed71869e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========> Train acc: 0.837 | Test acc: 0.828\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "    preds = model(inputs)\n",
    "    # preds = torch.argmax(preds, dim=-1)\n",
    "    preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  train_acc = accuracy(model, train_loader)\n",
    "  test_acc = accuracy(model, test_loader)\n",
    "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfFUkEM1ZWeG"
   },
   "source": [
    "Loss가 잘 떨어지고, 이전에 우리가 구현한 Transformer보다 더 빨리 수렴하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foks5u95ZQ1_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
